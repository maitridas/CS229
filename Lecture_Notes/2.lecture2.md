## Outline
1. Linear Regression
2. Batch/Stochastic Gradient
3. Normal Equation

## Process
1. Feed Training set to a learning algorithm
2. Generates a hypothesis which gives us the output

![Problem](./images/lecture2/img1.JPG)

## How do you represent the hypoyhetis
![Problem](./images/lecture2/img2.JPG)

![Problem](./images/lecture2/img3.JPG)

![Problem](./images/lecture2/img4.JPG)

![Problem](./images/lecture2/img5.JPG)

![Problem](./images/lecture2/img6.JPG)

![Problem](./images/lecture2/img7.JPG)

xi , yi -> ith training example

-  Okay. So, um, and I think the final bit of notation, um, I've been writing h of x as a function of the features of the house, as a function of the size and number of bedrooms of the house. Um, sometimes we emphasize that h depends both on the parameters Theta and on the input features x. Um, we're going to use h_Theta of x to emphasize that the hypothesis depends both on the parameters and on the, you know, input features x

![Problem](./images/lecture2/img8.JPG)

- This is J(theta)
- we add a 1/2 to make the calculation of derivative easier
- we minimize the difference
- Why square -> some reason for a generalisation because its kinda like a gaussian then.

## Gradient Descent 
- start with some theta (say theta is a vector of all zeros)
- keep changing theta to reduce J(theta)

![Problem](./images/lecture2/img9.JPG)

![Problem](./images/lecture2/img10.JPG)

- " a:= a+1" -> means assigning
- " a=b " -> asserting a = b
- alpha is learning rate

For only one example-

![Problem](./images/lecture2/img11.JPG)

![Problem](./images/lecture2/img12.JPG)

All term will be 0 in the partial derivative except thetaj

![Problem](./images/lecture2/img13.JPG)

![Problem](./images/lecture2/img14.JPG)

![Problem](./images/lecture2/img15.JPG)

![Problem](./images/lecture2/img16.JPG)

- alpha too large can overshoot the minima
- alpha too small too longer to train

![Problem](./images/lecture2/img17.JPG)